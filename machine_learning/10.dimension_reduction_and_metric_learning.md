# k 近邻学习
## 方法
给定测试样本，基于某种距离度量找出训练集中与其最靠近的 k 个训练样本，通过 k 个邻居的信息来预测
+ 一般使用多数投数法
+ 也可取 k 个样本的值的平均值作为预测值
+ 基于距离远近进行加权平均或加权投票
## 特点
懒惰学习（lazy learning）
+ 只是将样本保存起来，直到收到测试样本后再处理
+ 相反地，在训练阶段就对样本进行学习处理的方法成为急切学习 (urgent learning)
## 性能
+ k 是重要超参数
+ 使用不同距离计算方式也会得到不同结果
+ 最近邻分类器的泛化错误率不超过贝叶斯最优分类器的错误率的两倍
	预测样本 x 的错误率如下计算，取最近邻 z, $c^{*}$ 为贝叶斯最优分类器的预测
$$	
\begin{equation}
\begin{aligned}
	P(err) &= 1 - \sum_{c\in \gamma}P(c|x)(c|z)\\
	&\approx 1 - \sum_{c\in \gamma}P^{2}(c|x)\\
	&\leq 1 - P^{2}(c^{*}|x)\\\\
	&= (1 - P(c^{*}|x))(1+P(c^{*}|x))\\\\
	&\leq 2 \times (1 - P(c^{*}))\\
\end{aligned}
\end{equation}
$$

# 降维
## 动机
高维时出现维数灾难
+ 高维时难以进行密采样
+ 高维时难以计算距离（甚至难以计算内积）

与学习任务密切相关的可能只是某个低维分布，即高维空间的一个低维嵌入
## 线性降维
对 $X\in R^{d\times m}$ 施加线性变换 $W\in d\times d'$ 得到 $Z = W^{T}X$, 不同性质要求得到不同的 $W$
### MDS (多维缩放，Multiple Dimension Scaling)
#### 思想
任意两个样本在 d'维的欧式距离与原始空间的距离相等
#### 推导
设 $dist_{ij}$ 为原始矩阵中 $x_i, x_j$ 的距离，降维后得到 $Z\in R^{d'\times m}$ 
设 $B = Z^{T}Z\in R^{m\times m}$ 为降维后的内积矩阵, $b_{ij} = z_{i}^{T}z_{j}$

$$
\begin{equation}
\begin{aligned}
&dist_{ij}^{2} = |z_i|^{2} + |z_j|^{2} - 2z_{i}^{T}z_{j}
= b_{ii} + b_{jj} - 2b_{ij}\\
\end{aligned}
\end{equation}
$$
由 $\sum_{i=1}^{m}b_{ij}=\sum_{j=1}^{m}b_{ij}=0$（降维后的 $Z$ 已被中心化）得
$$
\begin{equation}
\begin{aligned}
&\sum_{i=1}^{m}dist_{ij}^{2} = tr(B) + mb_{ii}\\
&\sum_{j=1}^{m}dist_{ij}^{2} = tr(B) + mb_{jj}\\
&\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^{2} = 2mtr(B)\\
\end{aligned}
\end{equation}
$$
于是 
$$
b_{ij} = -\frac{1}{2}(dist_{ij}^{2} - \frac{1}{m}\sum_{i=1}^{m}dist_{ij}^{2} - \frac{1}{m}\sum_{j=1}^{m}dist_{ij}^{2} + \frac{1}{m^{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^{2})
$$
求得 B 后对 B 做特征值分解即可，取 d'个最大特征值, $\Lambda, V$ 分别为对应特征值/向量矩阵，$Z = \Lambda^{1/2}V$
### PCA (主成分分析, Principle Component Analysis)
#### 思想
+ 最近重构性：降维后与原始样本点的距离足够近
+ 最大可分性：降维后样本点之间尽可能分开
#### 推导
+ 最近重构性：
$$
\begin{equation}
\begin{aligned}
\sum_{i=1}^{m}|\sum_{j=1}^{d'}z_{ij}w_{j} - x_{i}|  &= \sum_{i=1}^{m}|Wz_{i}-x_{i}|^{2}\\
&= \sum_{i=1}^{m}|WW^{T}x_{i}-x_{i}|^{2}\\
&= \sum_{i=1}^{m}(x_{i}^{T}WW^{T}WW^{T}x_{i} - 2x_{i}^{T}WW^{T}x_{i} + x_{i}^{T}x_{i})\\
&= \sum_{i=1}^{m}(-x_{i}^{T}WW^{T}x_{i} + x_{i}^{T}x_{i})\\
&= \sum_{i=1}^{m}|WW^{T}x_{i}-x_{i}|^{2}\\
&= \sum_{i=1}^{m}-|W^{T}x_{i}|^{2} + x_{i}^{T}x_{i}\\
&= \alpha - \sum_{i=1}^{m}|W^{T}x_{i}|^{2} = \alpha - tr(W^TXX^TW)
\end{aligned}
\end{equation}
$$
于是优化目标为
$$
\begin{equation}
\begin{aligned}
&\min_{W} -tr(W^TXX^TW)\\
&s.t. W^TW = 1
\end{aligned}
\end{equation}
$$
+ 最大可分性：
要求投影后方差尽可能大，即协方差矩阵的 trace 尽可能大（推导详见南瓜书）
于是优化目标为
$$
\begin{equation}
\begin{aligned}
&\max_{W} tr(W^TXX^TW)\\
&s.t. W^TW = 1
\end{aligned}
\end{equation}
$$

+ 两者目标等价
使用拉格朗日乘子法可得
$$
XX^Tw_{i} = \lambda_iw_i
$$
对 $XX^T$ 进行特征值分解，取前 d'大的特征值对应的特征向量构成 $W^*=(w_1,w_2,...w_{d'})$ 即可
#### 超参数选择
+ 用户指定
+ 对 k 紧邻分类器进行交叉验证
+ 使用重构阈值 t（如 t=0.95）, 寻找最小 d'值满足下式
$$
\frac{\sum_{i=1}^{d'}\lambda_i}{\sum_{i=1}^{d}\lambda_i}\geq t
$$
#### 降维的作用
+ 增大样本的采样密度
+ 最小特征值对应的特征往往与噪声有关，可以去噪
### 核化线性降维 
#### 思想
需要非线性映射才能找到恰当的低维嵌入
- [?] 实际上是对核化后的特征进行降维???
#### 推导
由 PCA 可知
$$
\begin{equation}
\begin{aligned}
&(\sum_{i=1}^{m}z_iz_i^t)w_j = \lambda_jw_j\\
&w_j = \frac{1}{\lambda_j}(\sum_{i=1}^{m}z_iz_i^T)w_j=\sum_{i=1}^{m}z_i\frac{z_i^Tw_j}{\lambda_j}=\sum_{i=1}^{m}z_i\alpha_i^j
\end{aligned}
\end{equation}
$$
其中 $z_i$ 是 $x_i$ 在高维空间的像, 设 $z_i = \phi(x_i)$
第二行可理解为新坐标轴在原样本上作投影后还原, 由于 $z_{i}$ 未单位化，故添加系数 $\lambda_j$
考虑

于是可替换为
$$
(\sum_{i=1}^{m}\phi(x_i)\phi(x_i)^T)w_j = \lambda_jw_j
$$
其中 $w_j = \sum_{i=1}^{m}\phi(x_i)\alpha_i^j = Z\alpha^j, \alpha^{j} = (\alpha_1^j, \alpha_2^j, ...)$
于是
$$
ZZ^{T}Z\alpha^{j} = \lambda_jZ\alpha^j
$$
求解 $Z^TZ\alpha^j=\lambda_j\alpha^j$ 可满足上式，即 $\alpha^j$ 为 $K = Z^TZ$ 的特征向量, 其中 $K_{ij} = \kappa(x_i,x_j)$

对于新样本 x, 投影后第 j 维坐标为 $$
z_j = w_j^T\phi(x)
$$
## 流型学习
低维流型嵌入到高维空间中
### Isomap (等度量映射)
#### 思想
高维空间中的距离不应计算直线距离，而应计算"测地线" (geodesic)距离
#### 算法
+ 对每个点基于欧氏距离找出近邻点, 建立近邻连接图，只有近邻点之间连边
+ 计算近邻连接图上任意两点间最短路径, 从而近似测地线距离
+ 使用 MDS 在已知距离矩阵下计算出样本在低维空间的坐标

近邻图构建及伪代码详见书本 [[Machine_learning.pdf#page=234&offset=,,|Machine_learning, 10.5 流形学习]]
#### 缺点
只能计算已有样本的坐标, 无法将新样本映射到低维空间
### 局部线性嵌入
#### 思想
试图保持邻域内样本间的线性关系
#### 推导
样本点 $x_i$ 可由近邻下标集合 $Q_i$ 中的点进行线性组合得到, $w_i(w_{i1},w_{i2},...)$ 为线性重构系数
弱化为与线性组合的距离足够小，可得以下优化目标
$$
\begin{equation}
\begin{aligned}
&\min_{w_1,w_2,...,w_m}\sum_{i=1}^{m}|x_i-\sum_{j\in Q_i}w_{ij}x_j|^2\\
&s.t.\sum_{j\in Q_i}w_{ij} = 1
\end{aligned}
\end{equation}
$$
求得所有 $w_i$ 后需要求解
$$
\min_{z_1,z_2,...,z_m}\sum_{i=1}^{m}|z_i-\sum_{j\in Q_i}w_{ij}z_j|^2
$$
最终可化简为取 $M=(I-W^T)(I-W)$ **最小**的 d'各特征值对应的特征向量组成 $Z^T$
优化推导详见课本 [[Machine_learning.pdf#page=234&offset=,,|Machine_learning, 10.5 流形学习]]
# 度量学习
### 概念
#### 带权欧氏距离
$$
dist^2(x_i,x_j) = |x_i-x_j|^2 = w_1 * dist_{ij,1}^2 + w_2 * dist_{ij,2}^2 + ... = (x_i-x_j)^TW(x_i-x_j)
$$
W 为对角阵，可由学习得到
#### 马氏距离
考虑 W 为对角阵，即各向量（坐标轴）正交，但实际情况往往是属性之间是相关的（不正交）
将 W 替换为半正定对称矩阵 $M=PP^T$ 可得
$$
dist_{mah}^2(x_i,x_j) = (x_i-x_j)^TM(x_i,x_j) = |x_i-x_j|_M^2 = |P^Tx_i-P^Tx_j|^2
$$
可理解为 P 空间下欧氏距离
### 推导
对 M 进行学习，下面以优化 K 近邻分类器性能为目标进行讨论

将 K 近邻中的投票法改为概率投票法(即随机一个01之间小数，根据落到范围进行投票), $x_j$ 影响 $x_i$ 分类结果的概率为
$$
P_{ij} = \frac{exp(-|x_i-x_j|_M^2)}{\sum_lexp(-|x_i-x_l|_M^2)}
$$
以留一法(使用除了 $x_i$ 外所有样本估计 $x_i$ 的标记)正确率最大化为目标, $\Omega_{i}$ 为与 i 标记相同的样本集合
$$
p_i = \sum_{j\in\Omega_i}p_{ij}
$$
留一法取所有评价的平均, 此处所有正确率之和即可
$$
\sum_{i=1}^{m}p_i = \sum_{i=1}^{m}\sum_{j\in\Omega_i}p_{ij}
$$
化简后优化目标为$$
\max_{P}\sum_{i=1}^{m}\sum_{j\in\Omega_i}\frac{exp(-|P^Tx_i-P^Tx_j|)}{\sum_{l}exp(-|P^Tx_i-P^Tx_l|)}
$$
### 另一种目标
将与 $x_i$ 相似样本定义为 must-link(集合为 $\mathcal{M}$), 不相似样本定义为 cannot-link(集合为 $\mathcal{C}$), 求解下列凸优化问题
$$
\begin{equation}
\begin{aligned}
&\min_{M}\sum_{x_i,x_j\in\mathcal{M}}|x_i-x_j|_M^2\\
&s.t.\sum_{x_i,x_k\in\mathcal{C}}|x_i-x_k|_M\geq 1,\\
&M\succ 0\ \ \ (M半正定)
\end{aligned}
\end{equation}
$$
### 降维
M 可能是一个低秩矩阵，对 M 进行特征值分解可得到降维矩阵

可理解为 $P^Tx_i$ 中将 $P^T$ 替换成一个降维矩阵 